{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyspark in /home/mt/.local/lib/python3.10/site-packages (3.3.0)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /home/mt/.local/lib/python3.10/site-packages (from pyspark) (0.10.9.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pyarrow\n",
      "  Downloading pyarrow-9.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 35.3 MB 3.9 MB/s eta 0:00:01    |████▊                           | 5.2 MB 3.8 MB/s eta 0:00:08     |███████████████████████▍        | 25.8 MB 3.5 MB/s eta 0:00:03\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/lib64/python3.10/site-packages (from pyarrow) (1.21.5)\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-9.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.21.1-py3-none-any.whl (4.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.7 MB 5.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/lib/python3.10/site-packages (from transformers) (3.3.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/lib64/python3.10/site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3.10/site-packages (from transformers) (21.0)\n",
      "Collecting tqdm>=4.27\n",
      "  Downloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 2.5 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.9.0-py3-none-any.whl (120 kB)\n",
      "\u001b[K     |████████████████████████████████| 120 kB 7.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2022.8.17-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (766 kB)\n",
      "\u001b[K     |████████████████████████████████| 766 kB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.6 MB 6.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/lib/python3.10/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib64/python3.10/site-packages (from transformers) (5.4.1)\n",
      "Collecting typing-extensions>=3.7.4.3\n",
      "  Downloading typing_extensions-4.3.0-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/lib/python3.10/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3.10/site-packages (from requests->transformers) (1.26.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3.10/site-packages (from requests->transformers) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Installing collected packages: typing-extensions, tqdm, tokenizers, regex, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.9.0 regex-2022.8.17 tokenizers-0.12.1 tqdm-4.64.0 transformers-4.21.1 typing-extensions-4.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: torch in /home/mt/.local/lib/python3.10/site-packages (1.12.1+cpu)\n",
      "Requirement already satisfied: torchvision in /home/mt/.local/lib/python3.10/site-packages (0.13.1+cpu)\n",
      "Requirement already satisfied: torchaudio in /home/mt/.local/lib/python3.10/site-packages (0.12.1+cpu)\n",
      "Requirement already satisfied: typing-extensions in /home/mt/.local/lib/python3.10/site-packages (from torch) (4.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/mt/.local/lib/python3.10/site-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: requests in /usr/lib/python3.10/site-packages (from torchvision) (2.26.0)\n",
      "Requirement already satisfied: numpy in /usr/lib64/python3.10/site-packages (from torchvision) (1.21.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3.10/site-packages (from requests->torchvision) (1.26.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3.10/site-packages (from requests->torchvision) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/lib/python3.10/site-packages (from requests->torchvision) (2.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mt/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer  # Or BertTokenizer\n",
    "from transformers import AutoModelForPreTraining  # Or BertForPreTraining for loading pretraining heads\n",
    "from transformers import AutoModel  # or BertModel, for BERT without pretraining heads\n",
    "\n",
    "model = AutoModelForPreTraining.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
    "tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline('feature-extraction', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apache Spark version:  3.3.0\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=SparkSession.builder.appName(\"local[*]\").getOrCreate()\n",
    "print(\"Apache Spark version: \", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "import pyspark.pandas as ps\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mt/spark-3.3.0-bin-hadoop3/python/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_csv`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    }
   ],
   "source": [
    "sdf = ps.read_csv(\"semente.txt\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.rename(columns = {0:\"palavras\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mt/spark-3.3.0-bin-hadoop3/python/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    }
   ],
   "source": [
    "''''\n",
    "emb_list1= [float(x) for x in pipe(sdf['palavras'].iloc[0])[0][0]]\n",
    "emb_list2= [float(x) for x in pipe(sdf['palavras'].iloc[0])[0][1]]\n",
    "emb_list3= [float(x) for x in pipe(sdf['palavras'].iloc[0])[0][2]]\n",
    "emb_list4= [float(x) for x in pipe(sdf['palavras'].iloc[0])[0][3]]\n",
    "emb_list = [(w+x+y+z)/4 for w, x, y, z in zip(emb_list1,emb_list2,emb_list3,emb_list4)]\n",
    "\n",
    "embeddings = ps.DataFrame({'word': sdf['palavras'].iloc[0], 'word_embeddings': Vectors.dense(emb_list)}).to_spark()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 %\n"
     ]
    }
   ],
   "source": [
    "''''\n",
    "pct = -10\n",
    "\n",
    "for i, row in sdf.iterrows():\n",
    "    if i >= 10:\n",
    "        break\n",
    "    if i%10 == 0:\n",
    "        pct += 10\n",
    "        print(pct, '%')\n",
    "    emb = pipe(row['palavras'])\n",
    "    if len(emb[0]) == 4:\n",
    "        emb_list1= [float(x) for x in emb[0][0]]\n",
    "        emb_list2= [float(x) for x in emb[0][1]]\n",
    "        emb_list3= [float(x) for x in emb[0][2]]\n",
    "        emb_list4= [float(x) for x in emb[0][3]]\n",
    "        emb_list = [(w+x+y+z)/4 for w, x, y, z in zip(emb_list1,emb_list2,emb_list3,emb_list4)]\n",
    "    else:\n",
    "        emb_list1= [float(x) for x in emb[0][0]]\n",
    "        emb_list2= [float(x) for x in emb[0][1]]\n",
    "        emb_list3= [float(x) for x in emb[0][2]]\n",
    "        emb_list = [(w+x+y)/4 for w, x, y in zip(emb_list1,emb_list2,emb_list3)]\n",
    "\n",
    "\n",
    "    append_ps = ps.DataFrame({'word': row['palavras'], 'word_embeddings': Vectors.dense(emb_list)}).to_spark()\n",
    "    embeddings = embeddings.union(append_ps)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "vectorAssembler = VectorAssembler(inputCols = [\"word_embeddings\"], outputCol = \"features\")\n",
    "df = vectorAssembler.transform(embeddings)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_list1= [float(x) for x in pipe(sdf['palavras'].iloc[0])[0][0]]\n",
    "emb_list2= [float(x) for x in pipe(sdf['palavras'].iloc[0])[0][1]]\n",
    "emb_list3= [float(x) for x in pipe(sdf['palavras'].iloc[0])[0][2]]\n",
    "emb_list4= [float(x) for x in pipe(sdf['palavras'].iloc[0])[0][3]]\n",
    "emb_list = [(w+x+y+z)/4 for w, x, y, z in zip(emb_list1,emb_list2,emb_list3,emb_list4)]\n",
    "\n",
    "data = [(Vectors.dense(emb_list), 2.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 %\n"
     ]
    }
   ],
   "source": [
    "pct = -10\n",
    "\n",
    "for i, row in sdf.iterrows():\n",
    "    if i >= 10:\n",
    "        break\n",
    "    if i%10 == 0:\n",
    "        pct += 10\n",
    "        print(pct, '%')\n",
    "    emb = pipe(row['palavras'])\n",
    "    if len(emb[0]) == 4:\n",
    "        emb_list1= [float(x) for x in emb[0][0]]\n",
    "        emb_list2= [float(x) for x in emb[0][1]]\n",
    "        emb_list3= [float(x) for x in emb[0][2]]\n",
    "        emb_list4= [float(x) for x in emb[0][3]]\n",
    "        emb_list = [(w+x+y+z)/4 for w, x, y, z in zip(emb_list1,emb_list2,emb_list3,emb_list4)]\n",
    "    else:\n",
    "        emb_list1= [float(x) for x in emb[0][0]]\n",
    "        emb_list2= [float(x) for x in emb[0][1]]\n",
    "        emb_list3= [float(x) for x in emb[0][2]]\n",
    "        emb_list = [(w+x+y)/4 for w, x, y in zip(emb_list1,emb_list2,emb_list3)]\n",
    "    \n",
    "    data.append((Vectors.dense(emb_list), 2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data, [\"features\", \"weighCol\"])\n",
    "kmeans = KMeans().setK(5).setSeed(1)\n",
    "cluster_model = kmeans.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.45002824798918634\n",
      "Cluster Centers: \n",
      "[-5.46598846 -5.6079756  -6.5021596  ... -5.66714466 -4.35651743\n",
      " -5.70500028]\n",
      "[-7.0864588  -7.90321696 -7.70176697 ... -7.45909345 -6.67905378\n",
      " -8.0147984 ]\n",
      "[-4.91280055 -4.84524731 -4.75558007 ... -5.05621576 -4.32895207\n",
      " -5.36712654]\n",
      "[-5.63282096 -5.62252679 -5.38914275 ... -5.60925388 -4.91662037\n",
      " -5.95578969]\n",
      "[-8.3905133  -7.91605306 -9.25838494 ... -9.29055715 -7.68108529\n",
      " -9.05738187]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions = cluster_model.transform(df)\n",
    "\n",
    "# Evaluate clustering by computing Silhouette score\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "\n",
    "centers = cluster_model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = kmeans.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
